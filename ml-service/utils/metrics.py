"""
Prometheus metrics for ML Service.
Centralized metrics that can be imported from anywhere.
"""

from prometheus_client import Counter, Histogram, Gauge, Info

# =============================================================================
# APPLICATION INFO
# =============================================================================

ml_info = Info('pomodoro_ml', 'Pomodoro ML Service Information')

# =============================================================================
# PREDICTION METRICS
# =============================================================================

PREDICTION_REQUESTS = Counter(
    'ml_prediction_requests_total',
    'Total prediction requests',
    ['prediction_type']
)

PREDICTION_ERRORS = Counter(
    'ml_prediction_errors_total',
    'Total prediction errors',
    ['prediction_type', 'error_type']
)

PREDICTION_DURATION = Histogram(
    'ml_prediction_duration_seconds',
    'Duration of ML predictions',
    ['prediction_type'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

# =============================================================================
# AI (LLM) METRICS
# =============================================================================

AI_REQUESTS = Counter(
    'ml_ai_requests_total',
    'Total AI requests',
    ['provider', 'endpoint']
)

AI_REQUEST_DURATION = Histogram(
    'ml_ai_request_duration_seconds',
    'Duration of AI requests',
    ['provider', 'endpoint'],
    buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 120.0, 180.0]
)

AI_ERRORS = Counter(
    'ml_ai_errors_total',
    'Total AI errors',
    ['provider', 'endpoint', 'error_type']
)

AI_TOKENS_INPUT = Counter(
    'ml_ai_tokens_input_total',
    'Total input tokens used by AI',
    ['provider', 'model', 'endpoint']
)

AI_TOKENS_OUTPUT = Counter(
    'ml_ai_tokens_output_total',
    'Total output tokens generated by AI',
    ['provider', 'model', 'endpoint']
)

AI_COST_USD = Counter(
    'ml_ai_cost_usd_total',
    'Estimated cost in USD',
    ['provider', 'model']
)

# =============================================================================
# CACHE METRICS
# =============================================================================

CACHE_HITS = Counter(
    'ml_cache_hits_total',
    'Total cache hits',
    ['cache_type']
)

CACHE_MISSES = Counter(
    'ml_cache_misses_total',
    'Total cache misses',
    ['cache_type']
)

# =============================================================================
# DATABASE METRICS
# =============================================================================

SESSIONS_ANALYZED = Gauge(
    'ml_sessions_analyzed_count',
    'Number of sessions available for analysis'
)

# =============================================================================
# COST ESTIMATION
# =============================================================================

# Pricing per 1M tokens (as of 2025)
PRICING = {
    'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
    'gpt-4o': {'input': 2.50, 'output': 10.00},
    'gpt-4-turbo': {'input': 10.00, 'output': 30.00},
    'gpt-3.5-turbo': {'input': 0.50, 'output': 1.50},
    'deepseek-chat': {'input': 0.14, 'output': 0.28},
    'deepseek-coder': {'input': 0.14, 'output': 0.28},
    'glm-4.5-air': {'input': 0.15, 'output': 0.80},  # Z.ai GLM-4.5-Air
    'glm-4.5': {'input': 0.20, 'output': 1.10},       # Z.ai GLM-5 (full)
    'qwen2.5:0.5b': {'input': 0.0, 'output': 0.0},  # Local/free
    'qwen2.5:14b': {'input': 0.0, 'output': 0.0},   # Local/free
}


def estimate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
    """Estimate cost in USD for given token usage."""
    pricing = PRICING.get(model, {'input': 0.0, 'output': 0.0})
    cost = (input_tokens * pricing['input'] + output_tokens * pricing['output']) / 1_000_000
    return cost


def record_ai_usage(provider: str, model: str, endpoint: str,
                    input_tokens: int = 0, output_tokens: int = 0,
                    duration_seconds: float = 0, error: str = None):
    """Record AI usage metrics."""
    AI_REQUESTS.labels(provider=provider, endpoint=endpoint).inc()

    if duration_seconds > 0:
        AI_REQUEST_DURATION.labels(provider=provider, endpoint=endpoint).observe(duration_seconds)

    if error:
        AI_ERRORS.labels(provider=provider, endpoint=endpoint, error_type=error).inc()

    if input_tokens > 0:
        AI_TOKENS_INPUT.labels(provider=provider, model=model, endpoint=endpoint).inc(input_tokens)

    if output_tokens > 0:
        AI_TOKENS_OUTPUT.labels(provider=provider, model=model, endpoint=endpoint).inc(output_tokens)

    # Estimate and record cost
    if input_tokens > 0 or output_tokens > 0:
        cost = estimate_cost(model, input_tokens, output_tokens)
        if cost > 0:
            AI_COST_USD.labels(provider=provider, model=model).inc(cost)
